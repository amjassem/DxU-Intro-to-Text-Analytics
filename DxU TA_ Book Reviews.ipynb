{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DxU TA: Book Reviews.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP00JrhZtrDSiN09J2r2Uuo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Preparation:** Run (Select the cell and `CTRL+Enter` or `CMD+Enter`) the following code just so that the output looks better."],"metadata":{"id":"k8s4FoIb_esM"}},{"cell_type":"code","source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"metadata":{"id":"3TjJZ1r__ZnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Whenever you want to add extra cells, you can do that by clicking `+Code` in the upper-left corner, or by clicking `Esc` (stop editting cell) and `A` (add a cell above) or `B` (add a cell below)"],"metadata":{"id":"aIMrzWxbLb8H"}},{"cell_type":"markdown","source":["# BOOK REVIEWS\n","\n","Let's start with consumer review. We'll look into book reviews. The data set we'll use is a subset of Amazon review data prepared by Jianmo Ni (https://nijianmo.github.io/amazon/index.html).\n","\n","We can ask, for example, what terms are indicative of the review being positive and try to build a model for predicting whether a review is positive or negative.\n","\n","First, let's download the data.\n","\n","1. **Download the file using the following code**"],"metadata":{"id":"sRJpiHaFhg1V"}},{"cell_type":"code","source":["# The following code downloads the file from GitHub:\n","!wget https://raw.githubusercontent.com/amjassem/DxU-Intro-to-Text-Analytics/main/Data/reviews.json"],"metadata":{"id":"VAb-cqDWzb5H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see by the extension of the file, this is a **JSON**.  This is a popular way to store data. Each line stores the data for a single review. \n","\n","*But there's more to a review than just the text*. Each review has **attributes** (text, rating, date), which have **values**.\n","\n","2. **Let's read the first line and see what it looks like:**"],"metadata":{"id":"ppSrkG940puQ"}},{"cell_type":"code","source":["# Read a single line from a file\n","with open('reviews.json', 'r') as f:\n","  print(f.readline())"],"metadata":{"id":"M94Axo8GkO30"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's focus on the text of the review. For this, we'll use the `json` package. You can then easily see the values for each attribute.\n","\n","3. **Run the code below:**"],"metadata":{"id":"WaKwiFSg2SSU"}},{"cell_type":"code","source":["import json\n","\n","# Read a line\n","with open('reviews.json', 'r') as f:\n","  line = f.readline()\n","\n","# Parse it using json\n","review = json.loads(line)\n","\n","# Read the rating\n","rating = review['overall']\n","print('Rating:', rating)\n","\n","# Read the text\n","text = review['reviewText']\n","print(text)"],"metadata":{"id":"RK51fp1W5Ude"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 1: Natural Language Processing\n","\n","## 1.A Tokenization\n","\n","In order to use analyze the text using statistical methods we need to **pre-process** it.\n","\n","Many packages can do all the following steps at once (and actually compute it quite a bit faster) but it's nice to be able to do it yourself in case you want to do something non-standard.\n","\n","First of all, we'll **tokenize** the document. We want to split the long string into the smallest meaningfull bits - tokens - and discard all the things that are not informative.\n","\n","A lot of text manipulation is based on *regular expressions* (regex) - finding particular characters or sequences of characters (**pattern**) in the text. `re` package provides all the functionalities for this.\n","\n","You can find basics of regex [here](https://cheatography.com/davechild/cheat-sheets/regular-expressions/). For example:\n","\n","* `abc` will look for \"abc\" in the text,\n","* `[abc]` will look for \"a\", \"b\" or \"c\",\n","* `[^abc]` will look for **not** \"a\", \"b\" and \"c\",\n","* `[a-j]` will look for a letter between \"a\" and \"j\", similar for `[K-Z]` or `[4-7]`\n","* `a*` will look for 0 or more \"a\"s\n","* `a+` will look for 1 or more \"a\"s\n","* `\\s` is used for white space, `\\n` for a new line\n","* `.` - Any character except new line (\\n)\n","* You can combine the above (an more) to make arbitrarily complex patterns, eg. `[^\\s]+@[\\^s]+\\.[^\\s]+` would find e-mail adresses.\n","\n","We'll start by **splitting** the text using `re.split(pettern, string)`.\n","4. **Test a couple of patterns to split on.**"],"metadata":{"id":"UVNhZ7td6puZ"}},{"cell_type":"code","source":["import re\n","# Splits the text according to some pattern\n","print(re.split('', text))"],"metadata":{"id":"mZGn63MCUGkV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we're interested only in keeping words, it might make sense to split on anything that is not an alphabetic character\n","5. **Split the text into only words**"],"metadata":{"id":"v8SRTPM-Dp50"}},{"cell_type":"code","source":["# Splits the text on anything that is not letters\n","tokens = re.split('[^A-Za-z]+', text)\n","print(tokens)"],"metadata":{"id":"09TRAvaeVmRO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you want to remove a pattern from a text, you can also use `re.sub(pattern, '', string)` (it effectively replaces the pattern with an empty string)"],"metadata":{"id":"e9hVsGOkQke1"}},{"cell_type":"markdown","source":["## 1.B Cleaning the tokens - stopwords, stemming, lemmatization\n","\n","We've split the string into tokens, but those tokens are still quite messy. \n","\n","* Is it really informative to have \"King\" and \"king\" be different terms? We might want to just have everything in **lower case**.\n","* How informative is \"the\" really? Such terms are called **stopwords** and we might want to exclude them.\n","* Is \"order\" and \"orders\" a crucial distinction we need? We might **stem** or **lemmatize** the tokens.\n","\n","The exact approach to pre-processing will always depend on what features you need to consider in the text!\n","\n","For now, let's start by getting a list of stopwords. For this, we'll use the `nltk` package, usefull in all kinds of NLP tasks.\n","\n","6. **Run the following code to get the list of stopwords:**"],"metadata":{"id":"5ati9BjgEzv9"}},{"cell_type":"code","source":["import nltk\n","# Downloads and loads stopwords\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","# We only need the english ones\n","stops = stopwords.words('english')\n","print(stops)"],"metadata":{"id":"1EaAgxEId6tx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's move on to **stemming** and **lemmatization**. Once again, we can use `nltk` to do either, although for lemmatizing we do need to first download extra resources. To lemmatize a word we also need to know what **part-of-speech** (POS) it is.\n","\n","7. **Run the code below to prepare the stemmer and the lemmatizer:**"],"metadata":{"id":"pTz-SqsvJIew"}},{"cell_type":"code","source":["nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","# Initilize the stemmer and the lemmatizer\n","stemmer =  PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# Define a function to get the POS of a word (in correct format)\n","def pos(word):\n","    # There's different formats for POS\n","    # We want the one expected by the lemmatizer\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","                \n","    return tag_dict.get(tag, wordnet.NOUN)"],"metadata":{"id":"9ScF2dfBIOgL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["8. **Check the stemming and lemmatization results for the following:**\n","  * orders\n","  * ordering\n","  * orderly\n","  * wrote\n","  * nothing\n"],"metadata":{"id":"WpyAtCuLS9M9"}},{"cell_type":"code","source":["# Choose a word\n","word = ''\n","print('Word: ' + word)\n","\n","# Find the stem of a word\n","stem = stemmer.stem(word)\n","print('Stem: ' + stem)\n","\n","# Find the lemma of a word (given it's POS)\n","lemma = lemmatizer.lemmatize(word, pos(word))\n","print('Lemma: ' + lemma)"],"metadata":{"id":"-Ajb-Sh_dqdv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's prepare a function to tokenize the documents and clean the tokens all at once. It should:\n","\n","9. **Which parts of the code below achieves the following tasks?**\n","* Split a text into tokens\n","* Convert to lower case\n","* Exclude stopwords\n","* Exclude anything shorter than 3 characters\n","* Lemmatize the token\n","\n"],"metadata":{"id":"4Q0UShivUgin"}},{"cell_type":"code","source":["def CleanToken(token):\n","  \"\"\"Converts a token to lower-case and lemmatizes it\"\"\"\n","\n","  token = token.lower() # A\n","\n","  token = lemmatizer.lemmatize(token, pos(word)) # B\n","  return token\n","\n","def CleanDocument(text):\n","  \"\"\"Splits the document, cleans tokens, removed redundant\"\"\"\n","  \n","  tokens = re.split('[^A-Za-z]+', text) # C\n","\n","  tokens = [CleanToken(token) for token in tokens] # D\n","\n","  tokens = [token for token in tokens if token not in stops and len(token) >= 3] # E\n","  return tokens"],"metadata":{"id":"x6CWnV6ZipHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["10. **Let's see whether it works:**"],"metadata":{"id":"ej6LB7UrEjxI"}},{"cell_type":"code","source":["# Clean a document and print is\n","print(CleanDocument(text))"],"metadata":{"id":"lghGMkD9EjCn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have a function that tokenizes (and cleans a text) let's run it for all the review.\n","\n","We need to read each review JSON (line of the file), find the text and tokenize it.\n","\n","*If this takes to long, you can run the next code cell to download the pre-processed data:*\n","\n","11. a) **Run the following the pre-process the whole corpus:**"],"metadata":{"id":"DkAJFYJtlTuL"}},{"cell_type":"code","source":["corpus = []\n","ratings = []\n","\n","\n","f = open('reviews.json', 'r')\n","for line in f:\n","\n","  # Parse line and get the text review\n","  try:\n","    review = json.loads(line)\n","    text = review['reviewText']\n","  except:\n","    # Not all reviews have text, we just skip those\n","    continue\n","\n","  # Tokenize and clean\n","  doc = CleanDocument(text)\n","\n","  # Add to the corpus\n","  corpus.append(doc)\n","\n","  # Record the rating\n","  rating = review['overall']\n","  ratings.append(rating)\n","\n","f.close()\n","nDoc = len(corpus)\n","\n","print('#Docs:', nDoc, ', #Ratings:', len(ratings))"],"metadata":{"id":"NgE8ZNgslSNk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["11. b) **Run the following to download already pre-processed data**"],"metadata":{"id":"qXABIEJWnSsl"}},{"cell_type":"code","source":["# The following code downloads the pre-processed corpus from GitHub:\n","!wget https://raw.githubusercontent.com/amjassem/DxU-Intro-to-Text-Analytics/main/Data/reviews_corpus.pickle\n","\n","# The file is serialized, this unpacks it\n","import pickle\n","corpus = pickle.load(open('reviews_corpus.pickle', 'rb'))\n","\n","ratings = []\n","\n","f = open('reviews.json', 'r')\n","for line in f:\n","\n","  # Parse line and get the text review\n","  try:\n","    review = json.loads(line)\n","    text = review['reviewText']\n","  except:\n","    # Not all reviews have text, we just skip those\n","    continue\n","\n","  # Record the rating\n","  rating = review['overall']\n","  ratings.append(rating)\n","\n","f.close()\n","nDoc = len(corpus)\n","\n","print('#Docs:', nDoc, ', #Ratings:', len(ratings))"],"metadata":{"id":"FupVUCxBnQtZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.C Vectorization\n","\n","In the previous section we've defined the features of the text that interest us - what words are used.\n","\n","Still, a list of tokens is not going to work for many of the methods we want to use. We might want to **vectorize** our data - express it as a vector of numeric values.\n","\n","A common way to do that is to compute a TF-IDF:\n","* each element of the vector tells us how many times each term is used in a document\n","* but the values are scaled down by how common the term is across documents\n","\n","`sklearn` provides a lot of functionalities for statistics and machine learning, including text analytics. We'll use the `TfidfVectorizer`. It actually can perform tokenization as well, but since we've already done that we'll tell it to not do that (hence the use of `ident_func`).\n","\n","You can see what other specification you might include in the vectorizer by running `?TfidfVectorizer`. Perhaps it's worth using some of them, for example `min_df=`?\n","\n","12. **Run the follwing to define a vectorizer object:**"],"metadata":{"id":"Y2dsyTHUcZcC"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Returns the documents unchanged\n","def ident_func(doc):\n","    return doc\n","\n","# The object for vectorizing the corpus\n","vecTfidf = TfidfVectorizer(\n","    analyzer='word',\n","    tokenizer=ident_func,\n","    preprocessor=ident_func,\n","    token_pattern=None)"],"metadata":{"id":"5dNdrqW1VlOy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have our vectorizer object ready, we can transform the corpus.\n","\n","13. **Run the following to get the TF-IDF:**\n","* See how the document is represented"],"metadata":{"id":"j7k7uPaBdE1w"}},{"cell_type":"code","source":["# Calculates the tf-idf and count for each term in each document\n","tfidf = vecTfidf.fit_transform(corpus)\n","\n","# Save the vocabulary (an attribute of the vectorizer object)\n","vocabulary =  vecTfidf.get_feature_names_out()\n","print('Number of terms:', len(vocabulary))\n","print(vocabulary[0:100])\n","\n","# Print the count vector for the first document\n","print('TF-IDF of a doc')\n","d = 0\n","for i in tfidf[d].nonzero()[1]:\n","  print(tfidf[d, i], vocabulary[i])"],"metadata":{"id":"3gZRWMfFbuPO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part 2: Analysis\n","\n","We have finished our NLP. Now we can used the pre-processed data in statistical analysis. Let's see if we can understand what words are indicative of a high/low rating."],"metadata":{"id":"DJsW5IAS1446"}},{"cell_type":"markdown","source":["## 2.B Regression\n","\n","We can start by running a regression.\n","\n","14. **Why would running an OLS would not be a good idea?**\n","* The code below prints the size of our predictor matrix (# observations, # variables)\n"],"metadata":{"id":"dDR9GRTcJ-7G"}},{"cell_type":"code","source":["# Print the dimensions of the TF-IDF matrix\n","print(tfidf.shape)"],"metadata":{"id":"g7_Ax4niJ-Qs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It might be better to run **penalized regression**, `sklearn` can do many of those:\n","* `LinearRegression()` for OLS - let's not use this one\n","* `Lasso()`\n","* `Ridge()`\n","* `ElasticNet()`\n","* and many more. Run `dir(linear_model)` to print all the methods in a module.\n","\n","For penalized regression you might want to first standardize the data.\n","\n","\n","15. **Run one of those models (perhaps Lasso?) on our data:**"],"metadata":{"id":"XI8uzO-AK6Zm"}},{"cell_type":"code","source":["from sklearn import linear_model\n","from sklearn.preprocessing import StandardScaler\n","\n","# Scale the data so that it has equal st. dev.\n","scaler = StandardScaler()\n","X = scaler.fit_transform(tfidf)\n","\n","# Run a model\n","clf = linear_model.MODELNAME()\n","clf.fit(X, ratings)\n","\n","# Print the R^2:\n","score = clf.score(tfidf, ratings)\n","print('R^2:', score)\n","\n","# Print the number of selected variables:\n","nSelected = (clf.coef_ != 0).sum()\n","print('Variables selected:', nSelected)"],"metadata":{"id":"W24sPZAJLB1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The results above might be poor (especially for Lasso). That's because we're running the model on the default value of `alpha` (you can see the default values by running e.g. `?linear_model.Lasso`.\n","\n","Let's try to optimize it.\n","\n","The following code:\n","* Splits the data-set into training and testing subsets.\n","* Runs a model for each value of `alpha`\n","* Calculates the score based on the testing set\n","* `tqdm` is a package for printing progress bars for loops. Usefull when the code is taking a while.\n","\n","16. **Run the following and decide which alpha is best (you can try a different model)**\n","* You might want to select a different value of alpha - for very small values the model might run very long\n"],"metadata":{"id":"Akrl6Ny53D5A"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from tqdm.notebook import tqdm\n","\n","# Split the dataset into training and testing subsets\n","X_train, X_test, y_train, y_test = train_test_split(X, ratings,\n","                                                    test_size=0.33, random_state=1)\n","\n","# Select values of alpha\n","alphas = [1e-4, 1e-3, 1e-2, 1e-1]\n","\n","for alpha in tqdm(alphas):\n","\n","  # Run a model\n","  clf = linear_model.MODELNAME(alpha=alpha)\n","  clf.fit(X_train, y_train)\n","\n","  # Print the R^2:\n","  score = clf.score(X_test, y_test)\n","  print()\n","\n","  # Print the number of selected variables:\n","  nSelected = (clf.coef_ != 0).sum()\n","  print('alpha:', alpha, 'R^2:', score, 'Variables selected:', nSelected)"],"metadata":{"id":"py76g1jr6A2B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Based on an estimated model, we might want to print the terms with the strongest coefficients.\n","\n","17. **Re-reun the model on the whole data-set**"],"metadata":{"id":"P3qunXJkQyk4"}},{"cell_type":"code","source":["clf = linear_model.Lasso(alpha=1e-4)\n","clf.fit(tfidf, ratings)"],"metadata":{"id":"OgSflf5BRkjo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's find out what are the terms with the biggest coefficients.\n","\n","Coefficients of a model (`clf.coef_`) are stored as an array. We can use the `numpy` package to find the index of the elements with the smallest (most negative) values.\n","\n","18. **Run the following to find the most negative terms.**\n","* You can find the most positive terms by searching `-clf.coef_` (biggest terms become smallest)."],"metadata":{"id":"8H2zBzn-SzV3"}},{"cell_type":"code","source":["import numpy as np\n","\n","topN = 20\n","# Finds the index of topN terms with the smallest values\n","sel = np.argsort(clf.coef_)[0:topN]\n","\n","for i in sel:\n","  # Print the term and its coefficient\n","  print(vocabulary[i], clf.coef_[i])"],"metadata":{"id":"AwOVprWv-koI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.B Sentiment analysis\n","\n","In the previous section we've build a regression model to identify which terms are indicative of a negative/positive review. Perhaps it's a bit of an overkill?\n","\n","There are lexicons that can tell us whether a term is positive or negative, maybe they'd do better?\n","\n","`nltk` provides a way to get assign sentiment score to texts.\n","\n","19. **Run the following to prepare the analyzer:**"],"metadata":{"id":"_-cwZUjoKoxi"}},{"cell_type":"code","source":["nltk.download('vader_lexicon')\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","# Defines the object for sentiment analysis\n","sia = SentimentIntensityAnalyzer()"],"metadata":{"id":"8JK8ilMlKn2Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["20. **Try out sentiment analysis on a couple of words. You can even use phrases or sentences!**"],"metadata":{"id":"AWWHyQAYUz_9"}},{"cell_type":"code","source":["word = \"amazing\"\n","# Print the sentiment score\n","print(word, sia.polarity_scores(word))"],"metadata":{"id":"sIzNGyCIL7Ms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's run the sentiment analysis for our documents. The analyzer expects a string, so we'll convert the tokens back into one long string.\n","\n","21. **Run the following to calculate sentiment score for the documents:**"],"metadata":{"id":"ZMqiqlqOVOaj"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Initialize an array to store the scores\n","sentiments = np.zeros((nDoc, 2))\n","\n","for d, doc in enumerate(tqdm(corpus)):\n","  # Get the sentiment scores for a documents\n","  sentiment = sia.polarity_scores(\" \".join(doc))\n","  # Record the positive and negative scores\n","  sentiments[d, 0] = sentiment['pos']\n","  sentiments[d, 1] = sentiment['neg']"],"metadata":{"id":"Fmo7S0-HK9Cv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Can we explain the rating based on the sentiment of the words? This time we only have two variables, so we can use OLS.\n","\n","22. **Run the following model:**"],"metadata":{"id":"s9eWEhGOVyTj"}},{"cell_type":"code","source":["# Run a model\n","clf = linear_model.LinearRegression()\n","clf.fit(sentiments, ratings)\n","\n","# Print the R^2:\n","score = clf.score(sentiments, ratings)\n","print('R^2:', score)"],"metadata":{"id":"NAH6wXsgVw5w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Did we do better than with penalized regression?\n","\n","* Actually, the analyzer does it's own processing, and we've removed things such as \"not\". You could try running it on the original, unprocessed texts."],"metadata":{"id":"q2IBVlGvWZ14"}},{"cell_type":"markdown","source":["# Part 3: Topic modelling\n","\n","Sentiment analysis is a form of dimensionality reduction. Above, we've reduced our documents into two dimensions (positive and negative sentiments). There's also other dimensionality techniques we can try.\n","\n","One of them is **topic modelling**. Using LDA we can find the topics in an unsupervised fashion - we do not specify them, but instead we'll learn the best fitting topics from the data.\n","\n","For the case of customer reviews the topics might refer to e.g. product categories, aspects of the products, or even sentiments.\n","\n","Perhaps the usage of some of the topics in the review text is indicative of the rating?\n","\n","LDA doesn't use TF-IDF, but the actual counts.\n","\n","23. **Run the following to get document-terms count matrix**\n","* The size of the vocabulary impacts the estimation time. You might want to exclude rare words using `min_df=` to simplify the model."],"metadata":{"id":"570LNiB7c36U"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Prepare vectorizer\n","vecCount = CountVectorizer(\n","    analyzer='word',\n","    tokenizer=ident_func,\n","    preprocessor=ident_func,\n","    min_df=1,\n","    token_pattern=None)\n","\n","# Transform the corpus into counts\n","counts = vecCount.fit_transform(corpus)\n","\n","# Get the vocabulary\n","vocabulary = vecCount.get_feature_names_out()\n","\n","print('Size of the vocabulary:', len(vocabulary))\n"],"metadata":{"id":"tHqJvtSfe3l8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's run the LDA model. Select a number of topics to estimate.\n","\n","Usually you'd want to estimate the model until it converges (reaches a stable state). Unfortunatelly we don't have that much time, so let's run the model with just 10 iterations (it will still take some time, about 2-3 min.)."],"metadata":{"id":"Jshj0UvSYXFU"}},{"cell_type":"code","source":["from sklearn.decomposition import LatentDirichletAllocation\n","\n","# Define the number of topics to estimate\n","nTop = 10\n","\n","# Run the model, record the proportions\n","lda = LatentDirichletAllocation(n_components=nTop, max_iter=10)\n","proportions = lda.fit_transform(counts)"],"metadata":{"id":"bEFiu6HuCfQU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now see the estimated topics distributions, and the proportion of each topic in each document.\n","\n","24. **Run the following to see the top words in a given topic**"],"metadata":{"id":"O78xQfzPZCkb"}},{"cell_type":"code","source":["# Select a topic\n","k = 0\n","# Number of top terms\n","nTerm = 25\n","\n","# Find the nTop words\n","sel = np.argsort(-lda.components_[k])[0:nTerm]\n","for i in sel:\n","  print(vocabulary[i], lda.components_[k, i])"],"metadata":{"id":"Deoemu5tZWBd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The above way of printing topics is not very nice to look at.\n","\n","Instead, we might want to use a wordcloud - an image showing top term, with each term's size scaled by it's importance.\n","\n","25. **Run the following to see the estimated topics**\n","* Do they have a clear interpratation?\n","* Looking at the topics, should we perhaps have added some additional steps in pre-processing?"],"metadata":{"id":"RlKjFIjJZ3oh"}},{"cell_type":"code","source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","# Number of top terms\n","nTerm = 150\n","\n","# Creates subplots, one for each wordcloud\n","fig, axs = plt.subplots(nTop, figsize=(8, 4*nTop))\n","\n","for k in range(nTop):\n","  \n","  # Find the nTop words\n","  sel = np.argsort(-lda.components_[k])[0:nTerm]\n","  \n","  # Get it in form of (term, frequency)\n","  topic = [(vocabulary[i], lda.components_[k, i]) for i in sel]\n","  topic = dict(topic)\n","\n","  # Create the wordcloud\n","  wordcloud = WordCloud(prefer_horizontal=1).generate_from_frequencies(topic)\n","\n","  # Plot it\n","  axs[k].imshow(wordcloud)\n","  axs[k].axis(\"off\")"],"metadata":{"id":"eZWNmipDDYqm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["26. Try running a regression of `ratings` on the document-topic mixing proportions (we saved them as `proportions`)"],"metadata":{"id":"eumMRs9CbsqR"}},{"cell_type":"code","source":[""],"metadata":{"id":"rlDlurM5b8E3"},"execution_count":null,"outputs":[]}]}