{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DxU TA: Trump Tweets.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMuTEDoNr+vvg9GNyF1MAYI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Preparation:** Run (Select the cell and `CTRL+Enter` or `CMD+Enter`) the following code just so that the output looks better."],"metadata":{"id":"5RxANZZKKgte"}},{"cell_type":"code","source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"metadata":{"id":"L6MjI5HPKoiZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Whenever you want to add extra cells, you can do that by clicking `+Code` in the upper-left corner, or by clicking `Esc` (stop editting cell) and `A` (add a cell above) or `B` (add a cell below)"],"metadata":{"id":"wg4--ecFLAa6"}},{"cell_type":"markdown","source":["# TRUMP TWEETS\n","\n","President Trump was well known to communicate a lot through Twitter. His tweets got a varying level of engagement, measure either through *Favorites* or *Retweets*. Can we understand what got people engaged, based on the contents of the tweets?\n","\n","We'll use a subset of the data set prepared by Brendan of https://www.thetrumparchive.com/ , tweets ranging from 2015-06-15 (the day he announced his candidacy) to 2022-01-08 (the dat his account was suspended).\n","\n","1. **Download the file using the following code**"],"metadata":{"id":"nBHFt0I7MMcx"}},{"cell_type":"code","source":["# The following code downloads the file from GitHub:\n","!wget https://raw.githubusercontent.com/amjassem/DxU-Intro-to-Text-Analytics/main/Data/trump_tweets.csv"],"metadata":{"id":"BLYS5fmsN_Bl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This time our data is in a **.csv** format (the less fancy cousin of .xls) you're most likely familiar with it. It's basically a table of tweets and the meta-data.\n","\n","The `pandas` package is very convenient for working with tables. \n","2. **Read the data and see the first few rows**"],"metadata":{"id":"TSlb_xWROJaF"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Read columns 1, 5, 6 and 7. Parse the dates\n","tweetData = pd.read_csv('trump_tweets.csv', usecols=[1, 5, 6, 7], parse_dates=[3])\n","# Print top 20 rows\n","tweetData.head(20)"],"metadata":{"id":"CDXv7NJbOHns"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 1: Clean the texts\n","\n","Tweets can often be messy to analyze, before we move forward we need to clean them up a bit.\n","\n","Things to consider:\n","* Removing url links\n","* Removing account handles (@account) and emails\n","* Remove strings that contain numbers\n","* Removing hashtags?\n","\n","We might use `re.sub(pattern, '', string)` for this. You can use the [regex cheatsheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/).\n","\n","URLs can be fairly-reliably found using the patter `(http|ftp|https)[^\\s]*` which looks for http or ftp or https, followed by any number of non-space characters\n","\n","How about finding account handles?\n","\n","3. **Finish the function below such that it removes:**\n","  * URLs, \n","  * account handles\n","  * e-mails\n","  * strings containing numbers"],"metadata":{"id":"0i45ULeGTH5t"}},{"cell_type":"code","source":["import re\n","\n","def CleanText(text):\n","\n","  # Removes URLs\n","  text = re.sub('(http|ftp|https)[^\\s]*', '', text)\n","  # Removes account handles\n","  text = re.sub('', '', text)\n","  # Removes emails\n","  text = re.sub('', '', text)\n","  # Remove strings with numbers (e.g. \"401k\")\n","  text = re.sub('', '', text)\n","\n","  return text\n","\n","# Test on a couple of tweets\n","sel = [40, 95, 976, 32140]\n","\n","for i in sel:\n","  print(\"===================\")\n","  print(tweetData.text[i])\n","  print(CleanText(tweetData.text[i]))"],"metadata":{"id":"muiQee0qPH8w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 2: Vectorization\n","\n","In case of book reviews we performed tokenization ourselves.\n","\n","If we don't want to include any special rules, or if we already did all the \"custom\" changes (like above) we can use a package for this. This is often much faster to compute actually.\n","\n","We'll run `CountVectorizer()`. Notice that the vocabulary is still a bit messy. It can usually be improved by setting a minimum count for terms to be included using the `min_df=` argument in `CountVectorizer()`. \n","\n","You can find out more options (arguments) by running `?CountVectorizer`.\n","\n","4. **First run the code below**\n","  * Then set a minumum count, and re-run the code, see how the vocabulary changes?"],"metadata":{"id":"MMiISq35ZiRS"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Get the text of the tweets\n","texts = [CleanText(text) for text in tweetData.text]\n","\n","# The object for vectorizing the corpus\n","vecCounts = CountVectorizer(stop_words='english')\n","# Vectorize the corpus\n","counts = vecCounts.fit_transform(texts)\n","\n","# Print the vocabulary\n","vocabulary = vecCounts.get_feature_names_out()\n","print('Number of terms:', len(vocabulary))\n","print(vocabulary[0:100])"],"metadata":{"id":"POYpsRKUYIxd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 3: Topic modelling\n","\n","Let's run a topic model to try to find out what the tweets are about.\n","\n","Select a number of topics `n_components=` and the number of iterations `max_iter=` (for now, let's choose a small number, e.g. 10 - when you run an actual analysis it's best to stop through convergence).\n","\n","By setting `verbose=1` we can see the progress of the estimation.\n","\n","5. **Fill in the missing code and run it**"],"metadata":{"id":"YBMQvdQEn5Pd"}},{"cell_type":"code","source":["from sklearn.decomposition import LatentDirichletAllocation\n","from tqdm.notebook import tqdm\n","\n","# Initialize the model\n","lda = LatentDirichletAllocation(____, _____, verbose=1)\n","# Estimate the parameters\n","proportions = lda.fit_transform(____)"],"metadata":{"id":"VMJ_CojsmxtA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see what topics we estimated. A convenient way to do this is to create a wordcloud for each topic.\n","\n","6. **Run the following code.**\n","  * Do the topic distributions have a clear interpretation?"],"metadata":{"id":"AB1IYeXEmjoW"}},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","import numpy as np\n","from wordcloud import WordCloud\n","\n","nTerm = 150\n","\n","# Initialize the subplots\n","nTop = lda.n_components\n","fig, axs = plt.subplots(nTop, figsize=(8, 4*nTop))\n","\n","for k in range(nTop):\n","  \n","  # Find top terms for each topic\n","  sel = np.argsort(-lda.components_[k])[0:nTerm]\n","  topic = [(vocabulary[i], lda.components_[k, i]) for i in sel]\n","  topic = dict(topic)\n","\n","  # Create a wordcloud and plot it\n","  wordcloud = WordCloud(prefer_horizontal=1).generate_from_frequencies(topic)\n","  axs[k].set_title(\"Topic \" + str(k))\n","  axs[k].imshow(wordcloud)\n","  axs[k].axis(\"off\")"],"metadata":{"id":"N8NO0D08pi55"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Task 4: Predict retweets"],"metadata":{"id":"M0KZBE5o0ULR"}},{"cell_type":"markdown","source":["Can we perhaps use the estimated topics to predict the number of retweets/favorites? Try estimating a regression model.\n","\n","First however, it would make sense to transform the retweets.\n","\n","7. **Build a model of your choice using the methods we've used for book reviews.**"],"metadata":{"id":"mjIZD98Xqm9Q"}},{"cell_type":"code","source":["from sklearn import linear_model\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tqdm.notebook import tqdm\n","\n","retweets = np.log(tweetData.retweets + 1)\n","\n","# Build a suitable model"],"metadata":{"id":"cDN7x6Lv3zEX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once you've selected a suitable model, you can re-do the wordclouds but include in the title the estimated coefficient for that topic:\n","* `axs[k].set_title(\"Topic \" + str(k) + \"Coef.: \" + \"{:.2f}\".format(clf.coef_[i]))`\n","* You can also try printing them out in order based on the value of the coefficients."],"metadata":{"id":"vJWCIo3fsdtL"}},{"cell_type":"code","source":[""],"metadata":{"id":"nSUaFFyvsdCE"},"execution_count":null,"outputs":[]}]}